{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we will start by importing the data\n",
    "\n",
    "In the notebook we work with the ghost state approach. Therefore our dataframe will consist of all the data for the previous elections for all states, concatenated in a single dataframe.\n",
    "\n",
    "We will begin by reading the .csv files for the different states and concatenate them into a single dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the base dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = pd.concat([pd.read_csv('states/'+file) for file in os.listdir('states/')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_9</th>\n",
       "      <th>republican</th>\n",
       "      <th>Year</th>\n",
       "      <th>Rep_House_Prop</th>\n",
       "      <th>State</th>\n",
       "      <th>Result</th>\n",
       "      <th>rep_loyalty</th>\n",
       "      <th>popular_vote_percentage</th>\n",
       "      <th>density</th>\n",
       "      <th>RDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.797513</td>\n",
       "      <td>51.909088</td>\n",
       "      <td>51.657848</td>\n",
       "      <td>1</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>53.37</td>\n",
       "      <td>106.259013</td>\n",
       "      <td>93086.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.538311</td>\n",
       "      <td>38.412820</td>\n",
       "      <td>37.298773</td>\n",
       "      <td>0</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>45.65</td>\n",
       "      <td>106.259013</td>\n",
       "      <td>93086.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.056567</td>\n",
       "      <td>39.511030</td>\n",
       "      <td>39.876493</td>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>43.01</td>\n",
       "      <td>113.717367</td>\n",
       "      <td>100758.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.651345</td>\n",
       "      <td>35.969860</td>\n",
       "      <td>38.271617</td>\n",
       "      <td>1</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>37.45</td>\n",
       "      <td>113.717367</td>\n",
       "      <td>100758.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.538179</td>\n",
       "      <td>44.819709</td>\n",
       "      <td>45.546053</td>\n",
       "      <td>0</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.23</td>\n",
       "      <td>123.352989</td>\n",
       "      <td>108448.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    month_10   month_11    month_9  republican  Year  Rep_House_Prop    State  \\\n",
       "0  51.797513  51.909088  51.657848           1  1988        0.200000  Georgia   \n",
       "1  37.538311  38.412820  37.298773           0  1988        0.800000  Georgia   \n",
       "2  41.056567  39.511030  39.876493           0  1992        0.900000  Georgia   \n",
       "3  35.651345  35.969860  38.271617           1  1992        0.100000  Georgia   \n",
       "4  46.538179  44.819709  45.546053           0  1996        0.636364  Georgia   \n",
       "\n",
       "   Result  rep_loyalty  popular_vote_percentage     density        RDI  \n",
       "0       1          0.5                    53.37  106.259013   93086.74  \n",
       "1       0          0.5                    45.65  106.259013   93086.74  \n",
       "2       1          0.7                    43.01  113.717367  100758.19  \n",
       "3       0          0.7                    37.45  113.717367  100758.19  \n",
       "4       0          0.5                    49.23  123.352989  108448.50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(base_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into train and test data (test data 2020 observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = base_df[base_df.Year != 2020]\n",
    "test_df = base_df[base_df.Year == 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_9</th>\n",
       "      <th>republican</th>\n",
       "      <th>Year</th>\n",
       "      <th>Rep_House_Prop</th>\n",
       "      <th>State</th>\n",
       "      <th>Result</th>\n",
       "      <th>rep_loyalty</th>\n",
       "      <th>popular_vote_percentage</th>\n",
       "      <th>density</th>\n",
       "      <th>RDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.797513</td>\n",
       "      <td>51.909088</td>\n",
       "      <td>51.657848</td>\n",
       "      <td>1</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>53.37</td>\n",
       "      <td>106.259013</td>\n",
       "      <td>93086.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.538311</td>\n",
       "      <td>38.412820</td>\n",
       "      <td>37.298773</td>\n",
       "      <td>0</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>45.65</td>\n",
       "      <td>106.259013</td>\n",
       "      <td>93086.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.056567</td>\n",
       "      <td>39.511030</td>\n",
       "      <td>39.876493</td>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>43.01</td>\n",
       "      <td>113.717367</td>\n",
       "      <td>100758.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.651345</td>\n",
       "      <td>35.969860</td>\n",
       "      <td>38.271617</td>\n",
       "      <td>1</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>37.45</td>\n",
       "      <td>113.717367</td>\n",
       "      <td>100758.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.538179</td>\n",
       "      <td>44.819709</td>\n",
       "      <td>45.546053</td>\n",
       "      <td>0</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.23</td>\n",
       "      <td>123.352989</td>\n",
       "      <td>108448.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    month_10   month_11    month_9  republican  Year  Rep_House_Prop    State  \\\n",
       "0  51.797513  51.909088  51.657848           1  1988        0.200000  Georgia   \n",
       "1  37.538311  38.412820  37.298773           0  1988        0.800000  Georgia   \n",
       "2  41.056567  39.511030  39.876493           0  1992        0.900000  Georgia   \n",
       "3  35.651345  35.969860  38.271617           1  1992        0.100000  Georgia   \n",
       "4  46.538179  44.819709  45.546053           0  1996        0.636364  Georgia   \n",
       "\n",
       "   Result  rep_loyalty  popular_vote_percentage     density        RDI  \n",
       "0       1          0.5                    53.37  106.259013   93086.74  \n",
       "1       0          0.5                    45.65  106.259013   93086.74  \n",
       "2       1          0.7                    43.01  113.717367  100758.19  \n",
       "3       0          0.7                    37.45  113.717367  100758.19  \n",
       "4       0          0.5                    49.23  123.352989  108448.50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_9</th>\n",
       "      <th>republican</th>\n",
       "      <th>Year</th>\n",
       "      <th>Rep_House_Prop</th>\n",
       "      <th>State</th>\n",
       "      <th>Result</th>\n",
       "      <th>rep_loyalty</th>\n",
       "      <th>popular_vote_percentage</th>\n",
       "      <th>density</th>\n",
       "      <th>RDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46.657139</td>\n",
       "      <td>47.298710</td>\n",
       "      <td>47.170450</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.525</td>\n",
       "      <td>180.617755</td>\n",
       "      <td>163524.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>47.604381</td>\n",
       "      <td>48.407587</td>\n",
       "      <td>45.906192</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.460</td>\n",
       "      <td>180.617755</td>\n",
       "      <td>163524.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46.311941</td>\n",
       "      <td>47.027987</td>\n",
       "      <td>46.593060</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>46.525</td>\n",
       "      <td>197.170250</td>\n",
       "      <td>161835.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>48.855849</td>\n",
       "      <td>48.848577</td>\n",
       "      <td>47.920197</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>52.460</td>\n",
       "      <td>197.170250</td>\n",
       "      <td>161835.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>66.209900</td>\n",
       "      <td>62.336910</td>\n",
       "      <td>66.226278</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.525</td>\n",
       "      <td>5.796735</td>\n",
       "      <td>10593.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     month_10   month_11    month_9  republican  Year  Rep_House_Prop  \\\n",
       "16  46.657139  47.298710  47.170450           1  2020        0.714286   \n",
       "17  47.604381  48.407587  45.906192           0  2020        0.285714   \n",
       "16  46.311941  47.027987  46.593060           1  2020        0.785714   \n",
       "17  48.855849  48.848577  47.920197           0  2020        0.214286   \n",
       "12  66.209900  62.336910  66.226278           1  2020        1.000000   \n",
       "\n",
       "             State  Result  rep_loyalty  popular_vote_percentage     density  \\\n",
       "16         Georgia       1          1.0                   46.525  180.617755   \n",
       "17         Georgia       0          1.0                   52.460  180.617755   \n",
       "16  North Carolina       1          0.8                   46.525  197.170250   \n",
       "17  North Carolina       0          0.8                   52.460  197.170250   \n",
       "12         Wyoming       1          1.0                   46.525    5.796735   \n",
       "\n",
       "          RDI  \n",
       "16  163524.50  \n",
       "17  163524.50  \n",
       "16  161835.04  \n",
       "17  161835.04  \n",
       "12   10593.19  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.head())\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['Year', 'State'], axis = 1)\n",
    "test_df = test_df.drop(['Year', 'State'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_9</th>\n",
       "      <th>republican</th>\n",
       "      <th>Rep_House_Prop</th>\n",
       "      <th>Result</th>\n",
       "      <th>rep_loyalty</th>\n",
       "      <th>popular_vote_percentage</th>\n",
       "      <th>density</th>\n",
       "      <th>RDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.797513</td>\n",
       "      <td>51.909088</td>\n",
       "      <td>51.657848</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>53.37</td>\n",
       "      <td>106.259013</td>\n",
       "      <td>93086.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.538311</td>\n",
       "      <td>38.412820</td>\n",
       "      <td>37.298773</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>45.65</td>\n",
       "      <td>106.259013</td>\n",
       "      <td>93086.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.056567</td>\n",
       "      <td>39.511030</td>\n",
       "      <td>39.876493</td>\n",
       "      <td>0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>43.01</td>\n",
       "      <td>113.717367</td>\n",
       "      <td>100758.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.651345</td>\n",
       "      <td>35.969860</td>\n",
       "      <td>38.271617</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>37.45</td>\n",
       "      <td>113.717367</td>\n",
       "      <td>100758.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.538179</td>\n",
       "      <td>44.819709</td>\n",
       "      <td>45.546053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.23</td>\n",
       "      <td>123.352989</td>\n",
       "      <td>108448.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    month_10   month_11    month_9  republican  Rep_House_Prop  Result  \\\n",
       "0  51.797513  51.909088  51.657848           1        0.200000       1   \n",
       "1  37.538311  38.412820  37.298773           0        0.800000       0   \n",
       "2  41.056567  39.511030  39.876493           0        0.900000       1   \n",
       "3  35.651345  35.969860  38.271617           1        0.100000       0   \n",
       "4  46.538179  44.819709  45.546053           0        0.636364       0   \n",
       "\n",
       "   rep_loyalty  popular_vote_percentage     density        RDI  \n",
       "0          0.5                    53.37  106.259013   93086.74  \n",
       "1          0.5                    45.65  106.259013   93086.74  \n",
       "2          0.7                    43.01  113.717367  100758.19  \n",
       "3          0.7                    37.45  113.717367  100758.19  \n",
       "4          0.5                    49.23  123.352989  108448.50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_9</th>\n",
       "      <th>republican</th>\n",
       "      <th>Rep_House_Prop</th>\n",
       "      <th>Result</th>\n",
       "      <th>rep_loyalty</th>\n",
       "      <th>popular_vote_percentage</th>\n",
       "      <th>density</th>\n",
       "      <th>RDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46.657139</td>\n",
       "      <td>47.298710</td>\n",
       "      <td>47.170450</td>\n",
       "      <td>1</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.525</td>\n",
       "      <td>180.617755</td>\n",
       "      <td>163524.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>47.604381</td>\n",
       "      <td>48.407587</td>\n",
       "      <td>45.906192</td>\n",
       "      <td>0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.460</td>\n",
       "      <td>180.617755</td>\n",
       "      <td>163524.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46.311941</td>\n",
       "      <td>47.027987</td>\n",
       "      <td>46.593060</td>\n",
       "      <td>1</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>46.525</td>\n",
       "      <td>197.170250</td>\n",
       "      <td>161835.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>48.855849</td>\n",
       "      <td>48.848577</td>\n",
       "      <td>47.920197</td>\n",
       "      <td>0</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>52.460</td>\n",
       "      <td>197.170250</td>\n",
       "      <td>161835.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>66.209900</td>\n",
       "      <td>62.336910</td>\n",
       "      <td>66.226278</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.525</td>\n",
       "      <td>5.796735</td>\n",
       "      <td>10593.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     month_10   month_11    month_9  republican  Rep_House_Prop  Result  \\\n",
       "16  46.657139  47.298710  47.170450           1        0.714286       1   \n",
       "17  47.604381  48.407587  45.906192           0        0.285714       0   \n",
       "16  46.311941  47.027987  46.593060           1        0.785714       1   \n",
       "17  48.855849  48.848577  47.920197           0        0.214286       0   \n",
       "12  66.209900  62.336910  66.226278           1        1.000000       1   \n",
       "\n",
       "    rep_loyalty  popular_vote_percentage     density        RDI  \n",
       "16          1.0                   46.525  180.617755  163524.50  \n",
       "17          1.0                   52.460  180.617755  163524.50  \n",
       "16          0.8                   46.525  197.170250  161835.04  \n",
       "17          0.8                   52.460  197.170250  161835.04  \n",
       "12          1.0                   46.525    5.796735   10593.19  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.head())\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train_df, train_size = 0.8, random_state=56)\n",
    "X_train = train.drop('Result', axis=1)#.values\n",
    "y_train = train['Result']#.values\n",
    "X_val = val.drop('Result', axis=1)#.values\n",
    "y_val = val['Result']#.values\n",
    "X_test = test_df.drop('Result', axis=1)#.values\n",
    "y_test = test_df['Result']#.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling(X_train, X_val, y_train, y_val, models, scaler=True, Poly=1, PCA_comp=0):\n",
    "    # pca is the total variance explained required. 0 means that we do not want to perform pca\n",
    "    if 'State' in X_train.columns:\n",
    "        X_train = X_train.drop('State', axis=1)\n",
    "        X_val = X_val.drop('State', axis=1)\n",
    "    if 'Year' in X_train.columns:\n",
    "        X_train = X_train.drop('Year', axis=1)\n",
    "        X_val = X_val.drop('Year', axis=1)\n",
    "    if PCA_comp > 0 and not scaler:\n",
    "        raise ValueError('We must normalize before performing PCA')\n",
    "    if scaler:\n",
    "        # print('Normalizing the data ...')\n",
    "        scaler = MinMaxScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "    if PCA_comp > 0:\n",
    "        # print('Performing PCA ...')\n",
    "        pca = PCA(n_components=X_train.shape[1])\n",
    "        pca.fit(X_train)\n",
    "        X_train = pca.transform(X_train)\n",
    "        X_val = pca.transform(X_val)\n",
    "        total_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        for i, variance in enumerate(total_variance):\n",
    "            if variance > PCA_comp:\n",
    "                break \n",
    "        X_train, X_val = X_train[:, :i+1], X_val[:, :i+1]\n",
    "    if Poly > 1:\n",
    "        # print('Polynomial transformation ...')\n",
    "        poly_features = PolynomialFeatures(degree=Poly, include_bias=False).fit(X_train)\n",
    "        X_train = poly_features.transform(X_train)\n",
    "        X_val = poly_features.transform(X_val)\n",
    "    accuracies = []\n",
    "    auc = []\n",
    "    # print('Modelling and gathering the predictions ..')\n",
    "    for i, model in enumerate(models):\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_val)\n",
    "        accuracies.append(accuracy_score(y_val, predictions))\n",
    "        try:\n",
    "            auc.append(roc_auc_score(y_val, predictions))\n",
    "        except ValueError:\n",
    "            auc.append(1)\n",
    "    return models, accuracies, auc, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Logistic Regression\n",
    "- PolynomialFeatures = 1, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0.9\n",
    "- PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-7fe71a2d006a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodels_considered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mLogisticRegressionCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'saga'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodels_11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies_11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maucs_11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels_considered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPoly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCA_comp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=1000, penalty='l1', solver='saga')]\n",
    "models_11, accuracies_11, aucs_11, predictions_11 = modelling(X_train, X_val, y_train, y_val, models=models_considered, scaler=True, Poly=1, PCA_comp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0}: {1}\".format(models_11[0], accuracies_11[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga')]\n",
    "models_12, accuracies_12, aucs_12, predictions_12 = modelling(X_train, X_val, y_train, y_val, models=models_considered, scaler=True, Poly=2, PCA_comp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga'): 0.8860759493670886\n"
     ]
    }
   ],
   "source": [
    "print(\"{0}: {1}\".format(models_12[0], accuracies_12[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8860759493670886\n",
      "0.96\n",
      "0.96\n"
     ]
    }
   ],
   "source": [
    "poly_features = PolynomialFeatures(degree=2, include_bias=False).fit(X_train)\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train1 = scaler.transform(X_train)\n",
    "X_val1 = scaler.transform(X_val)\n",
    "X_test1 = scaler.transform(X_test)\n",
    "\n",
    "X_train_poly = poly_features.transform(X_train1)\n",
    "X_val_poly = poly_features.transform(X_val1)\n",
    "X_test_poly = poly_features.transform(X_test1)\n",
    "model = LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga').fit(X_train_poly, y_train)\n",
    "print(accuracy_score(y_val, model.predict(X_val_poly)))\n",
    "print(accuracy_score(y_test, model.predict(X_test_poly)))\n",
    "print(model.score(X_test_poly, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga')]\n",
    "models_13, accuracies_13, aucs_13, predictions_13 = modelling(X_train, X_val, y_train, y_val, models=models_considered, scaler=True, Poly=2, PCA_comp=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga'): 0.8860759493670886\n"
     ]
    }
   ],
   "source": [
    "print(\"{0}: {1}\".format(models_13[0], accuracies_13[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=1000, penalty='l1', solver='saga')]\n",
    "models_14, accuracies_14, aucs_14, predictions_14 = modelling(X_train, X_val, y_train, y_val, models=models_considered, scaler=True, Poly=1, PCA_comp=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionCV(max_iter=1000, penalty='l1', solver='saga'): 0.7911392405063291\n"
     ]
    }
   ],
   "source": [
    "print(\"{0}: {1}\".format(models_14[0], accuracies_14[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Decision Tree Classifier\n",
    "- PolynomialFeatures = 1, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0.9\n",
    "- PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.810126582278481\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.810126582278481\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.8544303797468354\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8734177215189873\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.879746835443038\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.8987341772151899\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.8734177215189873\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.879746835443038\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.8860759493670886\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_21, accuracies_21, aucs_21, predictions_21 = modelling(X_train, X_val, y_train, y_val, models=models_considered, scaler=True, Poly=1, PCA_comp=0)\n",
    "    print(\"{0}: {1}\".format(models_21[0], accuracies_21[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.810126582278481\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.810126582278481\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.8544303797468354\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8481012658227848\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.8544303797468354\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.8670886075949367\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.8544303797468354\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.8734177215189873\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.8670886075949367\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_22, accuracies_22, aucs_22, predictions_22 = modelling(X_train, X_val, y_train, y_val, models=models_considered, scaler=True, Poly=2, PCA_comp=0)\n",
    "    print(\"{0}: {1}\".format(models_22[0], accuracies_22[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.7911392405063291\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.8164556962025317\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.8227848101265823\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8354430379746836\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.8924050632911392\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.8481012658227848\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.8607594936708861\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.8481012658227848\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.8544303797468354\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_23, accuracies_23, aucs_23, predictions_23 = modelling(X_train, X_val, y_train, y_val, models=models_considered, scaler=True, Poly=2, PCA_comp=0.9)\n",
    "    print(\"{0}: {1}\".format(models_23[0], accuracies_23[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.7848101265822784\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.7848101265822784\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.8227848101265823\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8481012658227848\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.8481012658227848\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.8481012658227848\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.8227848101265823\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.8291139240506329\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.8544303797468354\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_24, accuracies_24, aucs_24, predictions_24 = modelling(X_train, X_val, y_train, y_val, models=models_considered, scaler=True, Poly=1, PCA_comp=0.9)\n",
    "    print(\"{0}: {1}\".format(models_24[0], accuracies_24[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Random Forrest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1728 candidates, totalling 3456 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done 530 tasks      | elapsed:   31.1s\n",
      "[Parallel(n_jobs=-1)]: Done 880 tasks      | elapsed:   53.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1330 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1880 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2530 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3280 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3456 out of 3456 | elapsed:  3.4min finished\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators_over = [50, 90, 110, 150, 200, 300, 400, 500]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth_over = [int(x) for x in np.linspace(1, 10, num=6)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [1, 2, 3]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 4, 5]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators_over,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth_over,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "\n",
    "               }\n",
    "# Use the random grid to search for best hyperparameters\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "random_model = GridSearchCV(estimator=rf, param_grid=random_grid, verbose=1,\n",
    "                                    scoring='accuracy',\n",
    "                                    n_jobs=-1, return_train_score=True, cv=2, refit=True)\n",
    "random_model.fit(X_train, y_train)\n",
    "dict_rf = cross_validate(RandomForestClassifier(**random_model.best_params_), X_train, y_train, cv=9, scoring='accuracy')\n",
    "rf_initial_accuracy = np.mean(dict_rf['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'max_depth': 10,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 110}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8904761904761904"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_initial_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    5.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  1.0 the CV score is  0.8746031746031746\n",
      "{'learning_rate': 0.1, 'n_estimators': 64}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    5.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  2.0 the CV score is  0.8285714285714286\n",
      "{'learning_rate': 0.01, 'n_estimators': 159}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    6.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  3.0 the CV score is  0.8428571428571429\n",
      "{'learning_rate': 0.1, 'n_estimators': 23}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    6.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  4.0 the CV score is  0.8746031746031746\n",
      "{'learning_rate': 1, 'n_estimators': 91}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    7.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  5.0 the CV score is  0.8682539682539683\n",
      "{'learning_rate': 1, 'n_estimators': 186}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    7.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  6.0 the CV score is  0.8253968253968255\n",
      "{'learning_rate': 0.01, 'n_estimators': 77}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    4.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  7.0 the CV score is  0.8714285714285714\n",
      "{'learning_rate': 1, 'n_estimators': 77}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 135 out of 150 | elapsed:    1.9s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  8.0 the CV score is  0.8714285714285714\n",
      "{'learning_rate': 1, 'n_estimators': 77}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  9.0 the CV score is  0.8380952380952381\n",
      "{'learning_rate': 0.01, 'n_estimators': 172}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  10.0 the CV score is  0.8253968253968255\n",
      "{'learning_rate': 0.001, 'n_estimators': 91}\n"
     ]
    }
   ],
   "source": [
    "depths = np.linspace(1,10,10)\n",
    "\n",
    "# Base Estimator\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 15)]\n",
    "# Learning Rate\n",
    "learning_rate = [1e-3,1e-2,1e-1,1,10]\n",
    "\n",
    "# Create the random grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'learning_rate': learning_rate}\n",
    "scores = []\n",
    "params_scaled = []\n",
    "for depth in depths:\n",
    "    # Create a based model\n",
    "    ada = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = depth))\n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(estimator = ada, param_grid = param_grid, cv = 2, n_jobs = -1, verbose = 2, scoring = 'accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    dict_boosting = cross_validate(AdaBoostClassifier(**grid_search.best_params_), X_train, y_train, cv=9, scoring='accuracy', n_jobs=-1)\n",
    "    boosting_accuracy = np.mean(dict_boosting['test_score'])\n",
    "    print('With components ', depth, 'the CV score is ', boosting_accuracy)\n",
    "    print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-do everything with cross-validation only, no splitting in the traning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv('states/'+file) for file in os.listdir('states/')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[base_df.Year != 2020]\n",
    "test_df = df[base_df.Year == 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['Year', 'State'], axis = 1)\n",
    "test_df = test_df.drop(['Year', 'State'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop('Result', axis=1)#.values\n",
    "y_train = train_df['Result']#.values\n",
    "X_test = test_df.drop('Result', axis=1)#.values\n",
    "y_test = test_df['Result']#.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling(X_train, y_train, models, scaler=True, Poly=1, PCA_comp=0):\n",
    "    # pca is the total variance explained required. 0 means that we do not want to perform pca\n",
    "    if 'State' in X_train.columns:\n",
    "        X_train = X_train.drop('State', axis=1)\n",
    "    if 'Year' in X_train.columns:\n",
    "        X_train = X_train.drop('Year', axis=1)\n",
    "    if PCA_comp > 0 and not scaler:\n",
    "        raise ValueError('We must normalize before performing PCA')\n",
    "    if scaler:\n",
    "        # print('Normalizing the data ...')\n",
    "        scaler = MinMaxScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "    if PCA_comp > 0:\n",
    "        # print('Performing PCA ...')\n",
    "        pca = PCA(n_components=X_train.shape[1])\n",
    "        pca.fit(X_train)\n",
    "        X_train = pca.transform(X_train)\n",
    "        total_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        for i, variance in enumerate(total_variance):\n",
    "            if variance > PCA_comp:\n",
    "                break \n",
    "        X_train = X_train[:, :i+1]\n",
    "    if Poly > 1:\n",
    "        # print('Polynomial transformation ...')\n",
    "        poly_features = PolynomialFeatures(degree=Poly, include_bias=False).fit(X_train)\n",
    "        X_train = poly_features.transform(X_train)\n",
    "    model_cv_accuracies = []\n",
    "    # print('Modelling and gathering the predictions ..')\n",
    "    for i, model in enumerate(models):\n",
    "        model_cv = cross_validate(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        model_cv_accuracy = np.mean(model_cv['test_score'])\n",
    "        model_cv_accuracies.append(model_cv_accuracy)\n",
    "    return models, model_cv_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Logistic Regression\n",
    "- PolynomialFeatures = 1, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0.9\n",
    "- PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8705877610255583]\n"
     ]
    }
   ],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=1000, penalty='l1', solver='saga')]\n",
    "models_11, cv_accuracies_11 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=1, PCA_comp=0)\n",
    "print(cv_accuracies_11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.879464645650246]\n"
     ]
    }
   ],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga')]\n",
    "models_12, cv_accuracies_12 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=2, PCA_comp=0)\n",
    "print(cv_accuracies_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.879464645650246\n",
      "0.96\n"
     ]
    }
   ],
   "source": [
    "poly_features = PolynomialFeatures(degree=2, include_bias=False).fit(X_train)\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train1 = scaler.transform(X_train)\n",
    "X_test1 = scaler.transform(X_test)\n",
    "\n",
    "X_train_poly = poly_features.transform(X_train1)\n",
    "X_test_poly = poly_features.transform(X_test1)\n",
    "model = LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga')\n",
    "print(np.mean(cross_validate(model, X_train_poly, y_train, cv=5, scoring='accuracy')['test_score']))\n",
    "model.fit(X_train_poly, y_train)\n",
    "print(model.score(X_test_poly, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8476900749818593]\n"
     ]
    }
   ],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga')]\n",
    "models_13, cv_accuracies_13 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=2, PCA_comp=0.9)\n",
    "print(cv_accuracies_13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7640248327017657]\n"
     ]
    }
   ],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga')]\n",
    "models_14, cv_accuracies_14 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=1, PCA_comp=0.9)\n",
    "print(cv_accuracies_14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Decision Tree Classifier\n",
    "- PolynomialFeatures = 1, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0.9\n",
    "- PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.8262033379021204\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.8262033379021204\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.8604611787470773\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8693058131097317\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.8451664919777473\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.8489881480287028\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.850253970813513\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.8527775538176247\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.8363137950495847\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_21, cv_accuracies_21 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=1, PCA_comp=0)\n",
    "    print(\"{0}: {1}\".format(models_21[0], cv_accuracies_21[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.8300088688220593\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.8300088688220593\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.8414254615818753\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8248730145932436\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.8286543578166572\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.843916794323954\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.8489639603321777\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.8400951382729985\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.8349995968717245\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_22, cv_accuracies_22 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=2, PCA_comp=0)\n",
    "    print(\"{0}: {1}\".format(models_22[0], cv_accuracies_22[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.7804402160767556\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.8032895267274046\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.8312021285172942\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8020075788115779\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.8209787954527131\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.815947754575506\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.8096025155204385\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.8184874627106347\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.815947754575506\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_23, cv_accuracies_23 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=2, PCA_comp=0.9)\n",
    "    print(\"{0}: {1}\".format(models_23[0], cv_accuracies_23[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.765339030879626\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.7589292913004918\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.7881157784406998\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8121664113520921\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.8147383697492543\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.8147061194872208\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.828670482947674\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.8197774731919697\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.8184794001451262\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_24, cv_accuracies_24 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=1, PCA_comp=0.9)\n",
    "    print(\"{0}: {1}\".format(models_24[0], cv_accuracies_24[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Random Forrest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1728 candidates, totalling 3456 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   51.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3456 out of 3456 | elapsed:  3.7min finished\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators_over = [50, 90, 110, 150, 200, 300, 400, 500]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth_over = [int(x) for x in np.linspace(1, 10, num=6)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [1, 2, 3]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 4, 5]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators_over,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth_over,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "\n",
    "               }\n",
    "# Use the random grid to search for best hyperparameters\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "random_model = GridSearchCV(estimator=rf, param_grid=random_grid, verbose=1,\n",
    "                                    scoring='accuracy',\n",
    "                                    n_jobs=-1, return_train_score=True, cv=2, refit=True)\n",
    "random_model.fit(X_train, y_train)\n",
    "dict_rf = cross_validate(RandomForestClassifier(**random_model.best_params_), X_train, y_train, cv=9, scoring='accuracy')\n",
    "rf_initial_accuracy = np.mean(dict_rf['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "0.8934169278996866\n"
     ]
    }
   ],
   "source": [
    "print(random_model.best_params_)\n",
    "print(rf_initial_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(bootstrap = True, max_depth = 8, max_features = 'sqrt', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 50)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    5.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  1.0 the CV score is  0.8871037965865551\n",
      "{'learning_rate': 1, 'n_estimators': 132}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    6.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  2.0 the CV score is  0.8503715314060141\n",
      "{'learning_rate': 0.1, 'n_estimators': 23}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    7.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  3.0 the CV score is  0.8377017299431092\n",
      "{'learning_rate': 0.01, 'n_estimators': 145}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    7.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  4.0 the CV score is  0.8377017299431092\n",
      "{'learning_rate': 0.01, 'n_estimators': 118}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    7.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  5.0 the CV score is  0.8694270289097874\n",
      "{'learning_rate': 0.1, 'n_estimators': 37}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    8.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  6.0 the CV score is  0.8858556832694764\n",
      "{'learning_rate': 1, 'n_estimators': 77}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    6.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  7.0 the CV score is  0.8921978404737025\n",
      "{'learning_rate': 1, 'n_estimators': 105}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  8.0 the CV score is  0.8807616393823291\n",
      "{'learning_rate': 1, 'n_estimators': 37}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  9.0 the CV score is  0.8845930570068501\n",
      "{'learning_rate': 0.1, 'n_estimators': 105}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  10.0 the CV score is  0.8402269824683617\n",
      "{'learning_rate': 0.01, 'n_estimators': 159}\n"
     ]
    }
   ],
   "source": [
    "depths = np.linspace(1,10,10)\n",
    "\n",
    "# Base Estimator\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 15)]\n",
    "# Learning Rate\n",
    "learning_rate = [1e-3,1e-2,1e-1,1,10]\n",
    "\n",
    "# Create the random grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'learning_rate': learning_rate}\n",
    "scores = []\n",
    "params_scaled = []\n",
    "for depth in depths:\n",
    "    # Create a based model\n",
    "    ada = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = depth))\n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(estimator = ada, param_grid = param_grid, cv = 2, n_jobs = -1, verbose = 2, scoring = 'accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    dict_boosting = cross_validate(AdaBoostClassifier(**grid_search.best_params_), X_train, y_train, cv=9, scoring='accuracy', n_jobs=-1)\n",
    "    boosting_accuracy = np.mean(dict_boosting['test_score'])\n",
    "    print('With components ', depth, 'the CV score is ', boosting_accuracy)\n",
    "    print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = 7), learning_rate = 1, n_estimators = 105).fit(X_train, y_train)\n",
    "ada.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv('states/'+file) for file in os.listdir('states/')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[base_df.Year != 2020]\n",
    "test_df = df[base_df.Year == 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['Year', 'State'], axis = 1)\n",
    "test_df = test_df.drop(['Year', 'State'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop('Result', axis=1)#.values\n",
    "y_train = train_df['Result']#.values\n",
    "X_test = test_df.drop('Result', axis=1)#.values\n",
    "y_test = test_df['Result']#.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_9</th>\n",
       "      <th>republican</th>\n",
       "      <th>Rep_House_Prop</th>\n",
       "      <th>rep_loyalty</th>\n",
       "      <th>popular_vote_percentage</th>\n",
       "      <th>density</th>\n",
       "      <th>RDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.797513</td>\n",
       "      <td>51.909088</td>\n",
       "      <td>51.657848</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>53.37</td>\n",
       "      <td>106.259013</td>\n",
       "      <td>93086.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.538311</td>\n",
       "      <td>38.412820</td>\n",
       "      <td>37.298773</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>45.65</td>\n",
       "      <td>106.259013</td>\n",
       "      <td>93086.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.056567</td>\n",
       "      <td>39.511030</td>\n",
       "      <td>39.876493</td>\n",
       "      <td>0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>43.01</td>\n",
       "      <td>113.717367</td>\n",
       "      <td>100758.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.651345</td>\n",
       "      <td>35.969860</td>\n",
       "      <td>38.271617</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>37.45</td>\n",
       "      <td>113.717367</td>\n",
       "      <td>100758.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.538179</td>\n",
       "      <td>44.819709</td>\n",
       "      <td>45.546053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.23</td>\n",
       "      <td>123.352989</td>\n",
       "      <td>108448.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    month_10   month_11    month_9  republican  Rep_House_Prop  rep_loyalty  \\\n",
       "0  51.797513  51.909088  51.657848           1        0.200000          0.5   \n",
       "1  37.538311  38.412820  37.298773           0        0.800000          0.5   \n",
       "2  41.056567  39.511030  39.876493           0        0.900000          0.7   \n",
       "3  35.651345  35.969860  38.271617           1        0.100000          0.7   \n",
       "4  46.538179  44.819709  45.546053           0        0.636364          0.5   \n",
       "\n",
       "   popular_vote_percentage     density        RDI  \n",
       "0                    53.37  106.259013   93086.74  \n",
       "1                    45.65  106.259013   93086.74  \n",
       "2                    43.01  113.717367  100758.19  \n",
       "3                    37.45  113.717367  100758.19  \n",
       "4                    49.23  123.352989  108448.50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_9</th>\n",
       "      <th>republican</th>\n",
       "      <th>Rep_House_Prop</th>\n",
       "      <th>rep_loyalty</th>\n",
       "      <th>popular_vote_percentage</th>\n",
       "      <th>density</th>\n",
       "      <th>RDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46.657139</td>\n",
       "      <td>47.298710</td>\n",
       "      <td>47.170450</td>\n",
       "      <td>1</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.525</td>\n",
       "      <td>180.617755</td>\n",
       "      <td>163524.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>47.604381</td>\n",
       "      <td>48.407587</td>\n",
       "      <td>45.906192</td>\n",
       "      <td>0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.460</td>\n",
       "      <td>180.617755</td>\n",
       "      <td>163524.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46.311941</td>\n",
       "      <td>47.027987</td>\n",
       "      <td>46.593060</td>\n",
       "      <td>1</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.8</td>\n",
       "      <td>46.525</td>\n",
       "      <td>197.170250</td>\n",
       "      <td>161835.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>48.855849</td>\n",
       "      <td>48.848577</td>\n",
       "      <td>47.920197</td>\n",
       "      <td>0</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.8</td>\n",
       "      <td>52.460</td>\n",
       "      <td>197.170250</td>\n",
       "      <td>161835.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>66.209900</td>\n",
       "      <td>62.336910</td>\n",
       "      <td>66.226278</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.525</td>\n",
       "      <td>5.796735</td>\n",
       "      <td>10593.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     month_10   month_11    month_9  republican  Rep_House_Prop  rep_loyalty  \\\n",
       "16  46.657139  47.298710  47.170450           1        0.714286          1.0   \n",
       "17  47.604381  48.407587  45.906192           0        0.285714          1.0   \n",
       "16  46.311941  47.027987  46.593060           1        0.785714          0.8   \n",
       "17  48.855849  48.848577  47.920197           0        0.214286          0.8   \n",
       "12  66.209900  62.336910  66.226278           1        1.000000          1.0   \n",
       "\n",
       "    popular_vote_percentage     density        RDI  \n",
       "16                   46.525  180.617755  163524.50  \n",
       "17                   52.460  180.617755  163524.50  \n",
       "16                   46.525  197.170250  161835.04  \n",
       "17                   52.460  197.170250  161835.04  \n",
       "12                   46.525    5.796735   10593.19  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train.head())\n",
    "display(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Defining an Early Stopping Condition\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 50)\n",
    "regularizer = 0.00015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 630 samples, validate on 158 samples\n",
      "Epoch 1/50\n",
      "630/630 - 1s - loss: 0.9394 - accuracy: 0.4952 - val_loss: 0.8899 - val_accuracy: 0.4747\n",
      "Epoch 2/50\n",
      "630/630 - 0s - loss: 0.8626 - accuracy: 0.4825 - val_loss: 0.7851 - val_accuracy: 0.4557\n",
      "Epoch 3/50\n",
      "630/630 - 0s - loss: 0.7792 - accuracy: 0.4667 - val_loss: 0.7226 - val_accuracy: 0.4747\n",
      "Epoch 4/50\n",
      "630/630 - 0s - loss: 0.7192 - accuracy: 0.4889 - val_loss: 0.6826 - val_accuracy: 0.5316\n",
      "Epoch 5/50\n",
      "630/630 - 0s - loss: 0.7033 - accuracy: 0.5111 - val_loss: 0.6488 - val_accuracy: 0.6329\n",
      "Epoch 6/50\n",
      "630/630 - 0s - loss: 0.6645 - accuracy: 0.5698 - val_loss: 0.6058 - val_accuracy: 0.7152\n",
      "Epoch 7/50\n",
      "630/630 - 0s - loss: 0.6276 - accuracy: 0.6190 - val_loss: 0.5557 - val_accuracy: 0.7405\n",
      "Epoch 8/50\n",
      "630/630 - 0s - loss: 0.6025 - accuracy: 0.6429 - val_loss: 0.5150 - val_accuracy: 0.7785\n",
      "Epoch 9/50\n",
      "630/630 - 0s - loss: 0.5643 - accuracy: 0.6746 - val_loss: 0.4782 - val_accuracy: 0.7848\n",
      "Epoch 10/50\n",
      "630/630 - 0s - loss: 0.5476 - accuracy: 0.7032 - val_loss: 0.4408 - val_accuracy: 0.7911\n",
      "Epoch 11/50\n",
      "630/630 - 0s - loss: 0.5415 - accuracy: 0.7000 - val_loss: 0.4188 - val_accuracy: 0.7975\n",
      "Epoch 12/50\n",
      "630/630 - 0s - loss: 0.5129 - accuracy: 0.7476 - val_loss: 0.4008 - val_accuracy: 0.8165\n",
      "Epoch 13/50\n",
      "630/630 - 0s - loss: 0.5221 - accuracy: 0.7333 - val_loss: 0.3885 - val_accuracy: 0.8165\n",
      "Epoch 14/50\n",
      "630/630 - 0s - loss: 0.5158 - accuracy: 0.7476 - val_loss: 0.3814 - val_accuracy: 0.8165\n",
      "Epoch 15/50\n",
      "630/630 - 0s - loss: 0.4940 - accuracy: 0.7524 - val_loss: 0.3658 - val_accuracy: 0.8291\n",
      "Epoch 16/50\n",
      "630/630 - 0s - loss: 0.4923 - accuracy: 0.7698 - val_loss: 0.3552 - val_accuracy: 0.8354\n",
      "Epoch 17/50\n",
      "630/630 - 0s - loss: 0.4720 - accuracy: 0.7841 - val_loss: 0.3471 - val_accuracy: 0.8418\n",
      "Epoch 18/50\n",
      "630/630 - 0s - loss: 0.4701 - accuracy: 0.7810 - val_loss: 0.3343 - val_accuracy: 0.8481\n",
      "Epoch 19/50\n",
      "630/630 - 0s - loss: 0.4675 - accuracy: 0.7889 - val_loss: 0.3272 - val_accuracy: 0.8481\n",
      "Epoch 20/50\n",
      "630/630 - 0s - loss: 0.4562 - accuracy: 0.8063 - val_loss: 0.3172 - val_accuracy: 0.8544\n",
      "Epoch 21/50\n",
      "630/630 - 0s - loss: 0.4602 - accuracy: 0.7794 - val_loss: 0.3162 - val_accuracy: 0.8544\n",
      "Epoch 22/50\n",
      "630/630 - 0s - loss: 0.4601 - accuracy: 0.8095 - val_loss: 0.3144 - val_accuracy: 0.8481\n",
      "Epoch 23/50\n",
      "630/630 - 0s - loss: 0.4528 - accuracy: 0.8222 - val_loss: 0.3110 - val_accuracy: 0.8481\n",
      "Epoch 24/50\n",
      "630/630 - 0s - loss: 0.4352 - accuracy: 0.8175 - val_loss: 0.3054 - val_accuracy: 0.8544\n",
      "Epoch 25/50\n",
      "630/630 - 0s - loss: 0.4322 - accuracy: 0.8254 - val_loss: 0.2991 - val_accuracy: 0.8671\n",
      "Epoch 26/50\n",
      "630/630 - 0s - loss: 0.4369 - accuracy: 0.8222 - val_loss: 0.2935 - val_accuracy: 0.8608\n",
      "Epoch 27/50\n",
      "630/630 - 0s - loss: 0.4211 - accuracy: 0.8175 - val_loss: 0.2893 - val_accuracy: 0.8734\n",
      "Epoch 28/50\n",
      "630/630 - 0s - loss: 0.4307 - accuracy: 0.8190 - val_loss: 0.2909 - val_accuracy: 0.8797\n",
      "Epoch 29/50\n",
      "630/630 - 0s - loss: 0.4213 - accuracy: 0.8222 - val_loss: 0.2854 - val_accuracy: 0.8734\n",
      "Epoch 30/50\n",
      "630/630 - 0s - loss: 0.4061 - accuracy: 0.8333 - val_loss: 0.2806 - val_accuracy: 0.8797\n",
      "Epoch 31/50\n",
      "630/630 - 0s - loss: 0.4305 - accuracy: 0.8270 - val_loss: 0.2780 - val_accuracy: 0.8797\n",
      "Epoch 32/50\n",
      "630/630 - 0s - loss: 0.4380 - accuracy: 0.8127 - val_loss: 0.2797 - val_accuracy: 0.8797\n",
      "Epoch 33/50\n",
      "630/630 - 0s - loss: 0.4552 - accuracy: 0.8254 - val_loss: 0.2775 - val_accuracy: 0.8861\n",
      "Epoch 34/50\n",
      "630/630 - 0s - loss: 0.4051 - accuracy: 0.8349 - val_loss: 0.2762 - val_accuracy: 0.8861\n",
      "Epoch 35/50\n",
      "630/630 - 0s - loss: 0.4189 - accuracy: 0.8238 - val_loss: 0.2722 - val_accuracy: 0.8924\n",
      "Epoch 36/50\n",
      "630/630 - 0s - loss: 0.4089 - accuracy: 0.8397 - val_loss: 0.2651 - val_accuracy: 0.8924\n",
      "Epoch 37/50\n",
      "630/630 - 0s - loss: 0.3916 - accuracy: 0.8429 - val_loss: 0.2633 - val_accuracy: 0.8861\n",
      "Epoch 38/50\n",
      "630/630 - 0s - loss: 0.4184 - accuracy: 0.8222 - val_loss: 0.2606 - val_accuracy: 0.8861\n",
      "Epoch 39/50\n",
      "630/630 - 0s - loss: 0.4075 - accuracy: 0.8460 - val_loss: 0.2584 - val_accuracy: 0.8924\n",
      "Epoch 40/50\n",
      "630/630 - 0s - loss: 0.3973 - accuracy: 0.8444 - val_loss: 0.2542 - val_accuracy: 0.8924\n",
      "Epoch 41/50\n",
      "630/630 - 0s - loss: 0.3958 - accuracy: 0.8349 - val_loss: 0.2526 - val_accuracy: 0.8987\n",
      "Epoch 42/50\n",
      "630/630 - 0s - loss: 0.3950 - accuracy: 0.8365 - val_loss: 0.2493 - val_accuracy: 0.8987\n",
      "Epoch 43/50\n",
      "630/630 - 0s - loss: 0.3635 - accuracy: 0.8540 - val_loss: 0.2468 - val_accuracy: 0.8924\n",
      "Epoch 44/50\n",
      "630/630 - 0s - loss: 0.3892 - accuracy: 0.8429 - val_loss: 0.2448 - val_accuracy: 0.8987\n",
      "Epoch 45/50\n",
      "630/630 - 0s - loss: 0.3968 - accuracy: 0.8476 - val_loss: 0.2418 - val_accuracy: 0.8987\n",
      "Epoch 46/50\n",
      "630/630 - 0s - loss: 0.3861 - accuracy: 0.8397 - val_loss: 0.2369 - val_accuracy: 0.8987\n",
      "Epoch 47/50\n",
      "630/630 - 0s - loss: 0.3928 - accuracy: 0.8508 - val_loss: 0.2357 - val_accuracy: 0.8987\n",
      "Epoch 48/50\n",
      "630/630 - 0s - loss: 0.3999 - accuracy: 0.8381 - val_loss: 0.2376 - val_accuracy: 0.8987\n",
      "Epoch 49/50\n",
      "630/630 - 0s - loss: 0.4091 - accuracy: 0.8333 - val_loss: 0.2390 - val_accuracy: 0.9051\n",
      "Epoch 50/50\n",
      "630/630 - 0s - loss: 0.4016 - accuracy: 0.8365 - val_loss: 0.2389 - val_accuracy: 0.9051\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential(name = 'ANN')\n",
    "\n",
    "model.add(tf.keras.layers.Dense(6, activation='relu', input_shape=(9,), name='hidden1', kernel_regularizer = tf.keras.regularizers.l1(regularizer)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(6, activation='relu', name='hidden2', kernel_regularizer = tf.keras.regularizers.l1(regularizer)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(4, activation='relu', name='hidden3', kernel_regularizer = tf.keras.regularizers.l1(regularizer)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'output'))\n",
    "\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=2, validation_split=0.2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/1 - 0s - loss: 0.1440 - accuracy: 0.9300\n",
      "Test accuracy=0.9300000071525574\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Test accuracy={test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 551 samples, validate on 237 samples\n",
      "Epoch 1/100\n",
      "551/551 [==============================] - 1s 2ms/sample - loss: 0.6569 - accuracy: 0.6388 - val_loss: 0.5806 - val_accuracy: 0.7173\n",
      "Epoch 2/100\n",
      "551/551 [==============================] - 0s 180us/sample - loss: 0.6185 - accuracy: 0.6733 - val_loss: 0.5525 - val_accuracy: 0.7553\n",
      "Epoch 3/100\n",
      "551/551 [==============================] - 0s 149us/sample - loss: 0.5883 - accuracy: 0.7060 - val_loss: 0.5285 - val_accuracy: 0.7848\n",
      "Epoch 4/100\n",
      "551/551 [==============================] - 0s 155us/sample - loss: 0.5619 - accuracy: 0.7332 - val_loss: 0.5084 - val_accuracy: 0.7932\n",
      "Epoch 5/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.5401 - accuracy: 0.7586 - val_loss: 0.4910 - val_accuracy: 0.7975\n",
      "Epoch 6/100\n",
      "551/551 [==============================] - 0s 149us/sample - loss: 0.5210 - accuracy: 0.7822 - val_loss: 0.4752 - val_accuracy: 0.8059\n",
      "Epoch 7/100\n",
      "551/551 [==============================] - 0s 140us/sample - loss: 0.5031 - accuracy: 0.7931 - val_loss: 0.4598 - val_accuracy: 0.8143\n",
      "Epoch 8/100\n",
      "551/551 [==============================] - 0s 145us/sample - loss: 0.4857 - accuracy: 0.7895 - val_loss: 0.4455 - val_accuracy: 0.8101\n",
      "Epoch 9/100\n",
      "551/551 [==============================] - 0s 137us/sample - loss: 0.4695 - accuracy: 0.8022 - val_loss: 0.4321 - val_accuracy: 0.8186\n",
      "Epoch 10/100\n",
      "551/551 [==============================] - 0s 150us/sample - loss: 0.4536 - accuracy: 0.8094 - val_loss: 0.4191 - val_accuracy: 0.8143\n",
      "Epoch 11/100\n",
      "551/551 [==============================] - 0s 142us/sample - loss: 0.4382 - accuracy: 0.8330 - val_loss: 0.4054 - val_accuracy: 0.8101\n",
      "Epoch 12/100\n",
      "551/551 [==============================] - 0s 151us/sample - loss: 0.4241 - accuracy: 0.8385 - val_loss: 0.3924 - val_accuracy: 0.8186\n",
      "Epoch 13/100\n",
      "551/551 [==============================] - 0s 144us/sample - loss: 0.4105 - accuracy: 0.8403 - val_loss: 0.3812 - val_accuracy: 0.8228\n",
      "Epoch 14/100\n",
      "551/551 [==============================] - 0s 138us/sample - loss: 0.3988 - accuracy: 0.8421 - val_loss: 0.3706 - val_accuracy: 0.8228\n",
      "Epoch 15/100\n",
      "551/551 [==============================] - 0s 148us/sample - loss: 0.3882 - accuracy: 0.8421 - val_loss: 0.3615 - val_accuracy: 0.8186\n",
      "Epoch 16/100\n",
      "551/551 [==============================] - 0s 138us/sample - loss: 0.3784 - accuracy: 0.8457 - val_loss: 0.3532 - val_accuracy: 0.8312\n",
      "Epoch 17/100\n",
      "551/551 [==============================] - 0s 143us/sample - loss: 0.3699 - accuracy: 0.8439 - val_loss: 0.3451 - val_accuracy: 0.8354\n",
      "Epoch 18/100\n",
      "551/551 [==============================] - 0s 140us/sample - loss: 0.3619 - accuracy: 0.8512 - val_loss: 0.3391 - val_accuracy: 0.8439\n",
      "Epoch 19/100\n",
      "551/551 [==============================] - 0s 154us/sample - loss: 0.3551 - accuracy: 0.8530 - val_loss: 0.3331 - val_accuracy: 0.8439\n",
      "Epoch 20/100\n",
      "551/551 [==============================] - 0s 141us/sample - loss: 0.3488 - accuracy: 0.8512 - val_loss: 0.3270 - val_accuracy: 0.8481\n",
      "Epoch 21/100\n",
      "551/551 [==============================] - 0s 152us/sample - loss: 0.3426 - accuracy: 0.8494 - val_loss: 0.3209 - val_accuracy: 0.8523\n",
      "Epoch 22/100\n",
      "551/551 [==============================] - 0s 139us/sample - loss: 0.3372 - accuracy: 0.8494 - val_loss: 0.3162 - val_accuracy: 0.8523\n",
      "Epoch 23/100\n",
      "551/551 [==============================] - 0s 147us/sample - loss: 0.3321 - accuracy: 0.8548 - val_loss: 0.3118 - val_accuracy: 0.8565\n",
      "Epoch 24/100\n",
      "551/551 [==============================] - 0s 150us/sample - loss: 0.3278 - accuracy: 0.8548 - val_loss: 0.3078 - val_accuracy: 0.8565\n",
      "Epoch 25/100\n",
      "551/551 [==============================] - 0s 147us/sample - loss: 0.3236 - accuracy: 0.8566 - val_loss: 0.3042 - val_accuracy: 0.8565\n",
      "Epoch 26/100\n",
      "551/551 [==============================] - 0s 145us/sample - loss: 0.3196 - accuracy: 0.8639 - val_loss: 0.3012 - val_accuracy: 0.8565\n",
      "Epoch 27/100\n",
      "551/551 [==============================] - 0s 141us/sample - loss: 0.3159 - accuracy: 0.8621 - val_loss: 0.2986 - val_accuracy: 0.8565\n",
      "Epoch 28/100\n",
      "551/551 [==============================] - 0s 152us/sample - loss: 0.3125 - accuracy: 0.8657 - val_loss: 0.2953 - val_accuracy: 0.8565\n",
      "Epoch 29/100\n",
      "551/551 [==============================] - 0s 137us/sample - loss: 0.3092 - accuracy: 0.8675 - val_loss: 0.2918 - val_accuracy: 0.8565\n",
      "Epoch 30/100\n",
      "551/551 [==============================] - 0s 146us/sample - loss: 0.3066 - accuracy: 0.8675 - val_loss: 0.2890 - val_accuracy: 0.8565\n",
      "Epoch 31/100\n",
      "551/551 [==============================] - 0s 146us/sample - loss: 0.3038 - accuracy: 0.8657 - val_loss: 0.2876 - val_accuracy: 0.8565\n",
      "Epoch 32/100\n",
      "551/551 [==============================] - 0s 140us/sample - loss: 0.3010 - accuracy: 0.8675 - val_loss: 0.2841 - val_accuracy: 0.8565\n",
      "Epoch 33/100\n",
      "551/551 [==============================] - 0s 168us/sample - loss: 0.2987 - accuracy: 0.8748 - val_loss: 0.2826 - val_accuracy: 0.8565\n",
      "Epoch 34/100\n",
      "551/551 [==============================] - 0s 147us/sample - loss: 0.2964 - accuracy: 0.8766 - val_loss: 0.2803 - val_accuracy: 0.8565\n",
      "Epoch 35/100\n",
      "551/551 [==============================] - 0s 145us/sample - loss: 0.2939 - accuracy: 0.8766 - val_loss: 0.2784 - val_accuracy: 0.8565\n",
      "Epoch 36/100\n",
      "551/551 [==============================] - 0s 158us/sample - loss: 0.2919 - accuracy: 0.8766 - val_loss: 0.2766 - val_accuracy: 0.8565\n",
      "Epoch 37/100\n",
      "551/551 [==============================] - 0s 138us/sample - loss: 0.2900 - accuracy: 0.8766 - val_loss: 0.2741 - val_accuracy: 0.8565\n",
      "Epoch 38/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2883 - accuracy: 0.8766 - val_loss: 0.2730 - val_accuracy: 0.8565\n",
      "Epoch 39/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2868 - accuracy: 0.8784 - val_loss: 0.2709 - val_accuracy: 0.8565\n",
      "Epoch 40/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2850 - accuracy: 0.8802 - val_loss: 0.2702 - val_accuracy: 0.8565\n",
      "Epoch 41/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2835 - accuracy: 0.8802 - val_loss: 0.2687 - val_accuracy: 0.8608\n",
      "Epoch 42/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2819 - accuracy: 0.8820 - val_loss: 0.2677 - val_accuracy: 0.8608\n",
      "Epoch 43/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2807 - accuracy: 0.8820 - val_loss: 0.2666 - val_accuracy: 0.8608\n",
      "Epoch 44/100\n",
      "551/551 [==============================] - 0s 137us/sample - loss: 0.2792 - accuracy: 0.8838 - val_loss: 0.2659 - val_accuracy: 0.8650\n",
      "Epoch 45/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2781 - accuracy: 0.8802 - val_loss: 0.2655 - val_accuracy: 0.8650\n",
      "Epoch 46/100\n",
      "551/551 [==============================] - 0s 137us/sample - loss: 0.2770 - accuracy: 0.8838 - val_loss: 0.2644 - val_accuracy: 0.8650\n",
      "Epoch 47/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2761 - accuracy: 0.8838 - val_loss: 0.2634 - val_accuracy: 0.8650\n",
      "Epoch 48/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2749 - accuracy: 0.8838 - val_loss: 0.2630 - val_accuracy: 0.8650\n",
      "Epoch 49/100\n",
      "551/551 [==============================] - 0s 150us/sample - loss: 0.2738 - accuracy: 0.8838 - val_loss: 0.2620 - val_accuracy: 0.8650\n",
      "Epoch 50/100\n",
      "551/551 [==============================] - 0s 138us/sample - loss: 0.2731 - accuracy: 0.8857 - val_loss: 0.2612 - val_accuracy: 0.8650\n",
      "Epoch 51/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2722 - accuracy: 0.8857 - val_loss: 0.2608 - val_accuracy: 0.8650\n",
      "Epoch 52/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2712 - accuracy: 0.8838 - val_loss: 0.2597 - val_accuracy: 0.8650\n",
      "Epoch 53/100\n",
      "551/551 [==============================] - 0s 130us/sample - loss: 0.2705 - accuracy: 0.8857 - val_loss: 0.2583 - val_accuracy: 0.8650\n",
      "Epoch 54/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2694 - accuracy: 0.8857 - val_loss: 0.2584 - val_accuracy: 0.8692\n",
      "Epoch 55/100\n",
      "551/551 [==============================] - 0s 136us/sample - loss: 0.2687 - accuracy: 0.8857 - val_loss: 0.2584 - val_accuracy: 0.8692\n",
      "Epoch 56/100\n",
      "551/551 [==============================] - 0s 137us/sample - loss: 0.2677 - accuracy: 0.8838 - val_loss: 0.2575 - val_accuracy: 0.8692\n",
      "Epoch 57/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2667 - accuracy: 0.8857 - val_loss: 0.2569 - val_accuracy: 0.8692\n",
      "Epoch 58/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2660 - accuracy: 0.8857 - val_loss: 0.2571 - val_accuracy: 0.8692\n",
      "Epoch 59/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2654 - accuracy: 0.8838 - val_loss: 0.2570 - val_accuracy: 0.8692\n",
      "Epoch 60/100\n",
      "551/551 [==============================] - 0s 131us/sample - loss: 0.2645 - accuracy: 0.8820 - val_loss: 0.2559 - val_accuracy: 0.8692\n",
      "Epoch 61/100\n",
      "551/551 [==============================] - 0s 130us/sample - loss: 0.2637 - accuracy: 0.8875 - val_loss: 0.2553 - val_accuracy: 0.8692\n",
      "Epoch 62/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2631 - accuracy: 0.8857 - val_loss: 0.2548 - val_accuracy: 0.8692\n",
      "Epoch 63/100\n",
      "551/551 [==============================] - 0s 153us/sample - loss: 0.2623 - accuracy: 0.8857 - val_loss: 0.2551 - val_accuracy: 0.8692\n",
      "Epoch 64/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2615 - accuracy: 0.8875 - val_loss: 0.2542 - val_accuracy: 0.8734\n",
      "Epoch 65/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2607 - accuracy: 0.8875 - val_loss: 0.2534 - val_accuracy: 0.8734\n",
      "Epoch 66/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2598 - accuracy: 0.8893 - val_loss: 0.2529 - val_accuracy: 0.8692\n",
      "Epoch 67/100\n",
      "551/551 [==============================] - 0s 131us/sample - loss: 0.2591 - accuracy: 0.8893 - val_loss: 0.2519 - val_accuracy: 0.8692\n",
      "Epoch 68/100\n",
      "551/551 [==============================] - 0s 136us/sample - loss: 0.2581 - accuracy: 0.8893 - val_loss: 0.2515 - val_accuracy: 0.8692\n",
      "Epoch 69/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2574 - accuracy: 0.8893 - val_loss: 0.2517 - val_accuracy: 0.8692\n",
      "Epoch 70/100\n",
      "551/551 [==============================] - 0s 132us/sample - loss: 0.2564 - accuracy: 0.8893 - val_loss: 0.2508 - val_accuracy: 0.8692\n",
      "Epoch 71/100\n",
      "551/551 [==============================] - 0s 131us/sample - loss: 0.2560 - accuracy: 0.8911 - val_loss: 0.2500 - val_accuracy: 0.8692\n",
      "Epoch 72/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2554 - accuracy: 0.8893 - val_loss: 0.2510 - val_accuracy: 0.8776\n",
      "Epoch 73/100\n",
      "551/551 [==============================] - 0s 131us/sample - loss: 0.2550 - accuracy: 0.8893 - val_loss: 0.2510 - val_accuracy: 0.8776\n",
      "Epoch 74/100\n",
      "551/551 [==============================] - 0s 129us/sample - loss: 0.2545 - accuracy: 0.8893 - val_loss: 0.2511 - val_accuracy: 0.8776\n",
      "Epoch 75/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2532 - accuracy: 0.8893 - val_loss: 0.2498 - val_accuracy: 0.8776\n",
      "Epoch 76/100\n",
      "551/551 [==============================] - 0s 136us/sample - loss: 0.2525 - accuracy: 0.8893 - val_loss: 0.2492 - val_accuracy: 0.8776\n",
      "Epoch 77/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2520 - accuracy: 0.8893 - val_loss: 0.2494 - val_accuracy: 0.8776\n",
      "Epoch 78/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2512 - accuracy: 0.8911 - val_loss: 0.2490 - val_accuracy: 0.8776\n",
      "Epoch 79/100\n",
      "551/551 [==============================] - 0s 137us/sample - loss: 0.2506 - accuracy: 0.8929 - val_loss: 0.2493 - val_accuracy: 0.8776\n",
      "Epoch 80/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2498 - accuracy: 0.8911 - val_loss: 0.2488 - val_accuracy: 0.8776\n",
      "Epoch 81/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2493 - accuracy: 0.8911 - val_loss: 0.2487 - val_accuracy: 0.8776\n",
      "Epoch 82/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2487 - accuracy: 0.8911 - val_loss: 0.2486 - val_accuracy: 0.8776\n",
      "Epoch 83/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2481 - accuracy: 0.8911 - val_loss: 0.2484 - val_accuracy: 0.8776\n",
      "Epoch 84/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2473 - accuracy: 0.8947 - val_loss: 0.2494 - val_accuracy: 0.8776\n",
      "Epoch 85/100\n",
      "551/551 [==============================] - 0s 130us/sample - loss: 0.2470 - accuracy: 0.8947 - val_loss: 0.2500 - val_accuracy: 0.8776\n",
      "Epoch 86/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2464 - accuracy: 0.8947 - val_loss: 0.2491 - val_accuracy: 0.8776\n",
      "Epoch 87/100\n",
      "551/551 [==============================] - 0s 132us/sample - loss: 0.2461 - accuracy: 0.8947 - val_loss: 0.2497 - val_accuracy: 0.8776\n",
      "Epoch 88/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2451 - accuracy: 0.8947 - val_loss: 0.2497 - val_accuracy: 0.8776\n",
      "Epoch 89/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2448 - accuracy: 0.8966 - val_loss: 0.2479 - val_accuracy: 0.8819\n",
      "Epoch 90/100\n",
      "551/551 [==============================] - 0s 132us/sample - loss: 0.2442 - accuracy: 0.8984 - val_loss: 0.2474 - val_accuracy: 0.8819\n",
      "Epoch 91/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2436 - accuracy: 0.8947 - val_loss: 0.2477 - val_accuracy: 0.8819\n",
      "Epoch 92/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2432 - accuracy: 0.8947 - val_loss: 0.2480 - val_accuracy: 0.8819\n",
      "Epoch 93/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2424 - accuracy: 0.8966 - val_loss: 0.2477 - val_accuracy: 0.8819\n",
      "Epoch 94/100\n",
      "551/551 [==============================] - 0s 129us/sample - loss: 0.2419 - accuracy: 0.8966 - val_loss: 0.2473 - val_accuracy: 0.8819\n",
      "Epoch 95/100\n",
      "551/551 [==============================] - 0s 131us/sample - loss: 0.2414 - accuracy: 0.8966 - val_loss: 0.2472 - val_accuracy: 0.8819\n",
      "Epoch 96/100\n",
      "551/551 [==============================] - 0s 132us/sample - loss: 0.2411 - accuracy: 0.8966 - val_loss: 0.2467 - val_accuracy: 0.8819\n",
      "Epoch 97/100\n",
      "551/551 [==============================] - 0s 132us/sample - loss: 0.2405 - accuracy: 0.8984 - val_loss: 0.2465 - val_accuracy: 0.8819\n",
      "Epoch 98/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2400 - accuracy: 0.9002 - val_loss: 0.2459 - val_accuracy: 0.8819\n",
      "Epoch 99/100\n",
      "551/551 [==============================] - 0s 129us/sample - loss: 0.2393 - accuracy: 0.9002 - val_loss: 0.2451 - val_accuracy: 0.8819\n",
      "Epoch 100/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2392 - accuracy: 0.9002 - val_loss: 0.2443 - val_accuracy: 0.8819\n"
     ]
    }
   ],
   "source": [
    "model_simple = tf.keras.models.Sequential(name = 'ANN')\n",
    "\n",
    "model_simple.add(tf.keras.layers.Dense(6, activation='relu', input_shape=(9,), name='hidden1'))\n",
    "model_simple.add(tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'output'))\n",
    "\n",
    "model_simple.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "history = model_simple.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/1 - 0s - loss: 0.0920 - accuracy: 0.9400\n",
      "Test accuracy=0.9399999976158142\n"
     ]
    }
   ],
   "source": [
    "test_loss1, test_accuracy1 = model_simple.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Test accuracy={test_accuracy1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
